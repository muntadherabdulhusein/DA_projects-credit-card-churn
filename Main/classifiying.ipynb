{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../cleaned_data/cleaned_data.csv')\n",
    "\n",
    "display(data.describe())\n",
    "display(data.head())\n",
    "\n",
    "\n",
    "# Plotting histograms for numerical features\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i, col in enumerate(data.drop(['Attrition_Flag'], axis=1).select_dtypes(include=['int','float']).columns):\n",
    "    ax = plt.subplot(4, 4, i+1)  # Creating a subplot for each column.\n",
    "    sns.histplot(data=data, x=col, ax=ax, color='red', kde=True)\n",
    "    ax.tick_params(axis='x', labelsize=14)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.set_xlabel(col, fontsize=18)\n",
    "    ax.set_ylabel('Count', fontsize=18)\n",
    "plt.suptitle('Data distribution of continuous variables',fontsize=24, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between numerical features: heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(data.select_dtypes(include=['int', 'float']).corr(), annot=True, center=0,cmap='viridis',annot_kws={'size': 12})\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Feature correlation\",fontsize=24, y=1.01)\n",
    "plt.show()\n",
    "\n",
    "# Plot categorical features\n",
    "columns_to_visualize = ['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, column in enumerate(columns_to_visualize, 1):\n",
    "    plt.subplot(2, 3, i)  # 2 rows, 3 columns of subplots\n",
    "    sns.countplot(x=column, data=data, hue='Attrition_Flag', palette='viridis', legend=False)\n",
    "    plt.title(f'Distribution of {column} vs Attrition')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping\n",
    "plt.show()\n",
    "\n",
    "# Encoding ordinal features\n",
    "columns = ['Income_Category','Card_Category', 'Education_Level','Marital_Status']\n",
    "for col in columns:\n",
    "  data = pd.concat([data,pd.get_dummies(data[col], drop_first=True).astype(int)], axis=1)\n",
    "data.drop(columns=columns, inplace=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Prepare and split data\n",
    "X = data.drop(columns='Attrition_Flag').to_numpy()\n",
    "y = data['Attrition_Flag'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Initialize the base models\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define the models for comparison\n",
    "models = [\n",
    "    ('XGBoost', xgb_model),\n",
    "    ('Random Forest', rf_model),\n",
    "    ('Voting Classifier', VotingClassifier(estimators=[('xgb', xgb_model), ('rf', rf_model)], voting='soft'))\n",
    "]\n",
    "\n",
    "# Store the model evaluation results\n",
    "model_comparison = {}\n",
    "\n",
    "# Train and evaluate models, calculate results\n",
    "for model_name, classifier in models:\n",
    "    # Fit the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Make predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # Calculate score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    # Cross-validation\n",
    "    cross_val_accuracy = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5, scoring=\"accuracy\")\n",
    "    cv_accuracy = cross_val_accuracy.mean()\n",
    "    cv_std = cross_val_accuracy.std()\n",
    "    # Accuracy\n",
    "    accuracy_class_0 = accuracy_score(y_test[y_test == 0], y_pred[y_test == 0])\n",
    "    accuracy_class_1 = accuracy_score(y_test[y_test == 1], y_pred[y_test == 1])\n",
    "    # Print metrics\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Model F1-Score: {f1 * 100:.2f}%\")\n",
    "    print(f\"Cross Val Accuracy: {cv_accuracy * 100:.2f}%\")\n",
    "    print(f\"Cross Val Standard Deviation: {cv_std * 100:.2f}%\")\n",
    "    print(f\"Accuracy for Class 0: {accuracy_class_0 * 100:.2f}%\")\n",
    "    print(f\"Accuracy for Class 1: {accuracy_class_1 * 100:.2f}%\")\n",
    "\n",
    "    # Add metrics to model_comparison\n",
    "    model_comparison[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_class_0': accuracy_class_0,\n",
    "        'accuracy_class_1': accuracy_class_1,\n",
    "        'f1_score': f1,\n",
    "        'cv_accuracy': cv_accuracy,\n",
    "        'cv_std': cv_std\n",
    "    }\n",
    "    # Print classification report\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    #MODEL COMPARISSON\n",
    "\n",
    "Model_com_df=pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns=['Model F1-Score','Model Accuracy','Model Accuracy-0','Model Accuracy-1','CV Accuracy','CV std']\n",
    "Model_com_df=Model_com_df.sort_values(by='Model F1-Score',ascending=False)\n",
    "Model_com_df.style.format(\"{:.2%}\")\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion matrix', fontsize=18)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks)\n",
    "    plt.yticks(tick_marks)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n",
    "                 fontsize=16, color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=15)\n",
    "    plt.xlabel('Predicted label', fontsize=15)\n",
    "\n",
    "# Confusion matrix and plotting\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "plot_confusion_matrix(cm, classes=['y=0','y=1'], normalize=True)\n",
    "\n",
    "Model_com_df=pd.DataFrame(model_comparison).T\n",
    "Model_com_df.columns=['Model F1-Score','Model Accuracy','Model Accuracy-0','Model Accuracy-1','CV Accuracy','CV std']\n",
    "Model_com_df=Model_com_df.sort_values(by='Model F1-Score',ascending=False)\n",
    "Model_com_df.style.format(\"{:.2%}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
